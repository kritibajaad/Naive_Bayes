{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3103c04c",
   "metadata": {},
   "source": [
    "Before you turn this problem in, make sure everything runs as expected. First, **restart the kernel** (in the menubar, select Kernel $\\rightarrow$ Restart) and then **run all cells** (in the menubar, select Cell $\\rightarrow$ Run All).\n",
    "\n",
    "Make sure you fill in any place that says `YOUR CODE HERE` or \"YOUR ANSWER HERE\", as well as your name and collaborators below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e71f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME = \"Sanskriti Bajaad\"\n",
    "COLLABORATORS = \"Individual\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c8ef10",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace6ea2",
   "metadata": {},
   "source": [
    "# Naive Bayes Spam Detection \n",
    "\n",
    "Will build a Naive Bayes text classifier from scratch to detect spam messages. We will use a dataset of SMS messages labeled as \"ham\" (not spam) or \"spam\". The assignment will guide you through loading data, preprocessing text, calculating probabilities for Naive Bayes, and evaluating the classifier's performance. Each step is designed as a question that you will answer with code or written response. Make sure to follow instructions closely and fill in the required code where prompted. \n",
    "\n",
    "Dataset: We will use the SMS Spam Collection dataset​, a corpus of SMS messages classified as ham or spam. The dataset is provided as a CSV file (spam.csv) with two columns: one for the label (ham or spam) and one for the message text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604b3efd",
   "metadata": {},
   "source": [
    "## Question 1: Loading and Exploring the Dataset\n",
    "First, we need to load the spam dataset and get an understanding of its contents. This will involve reading the data from the file and checking basic statistics. Your tasks:\n",
    "Load the dataset from the provided file (e.g., spam.csv) into a pandas DataFrame.\n",
    "Ensure the data is read correctly (handle any encoding issues if necessary).\n",
    "Display the first 5 rows of the DataFrame to see the format of the data.\n",
    "Compute the number of messages that are ham vs. spam and print these counts.\n",
    "This will give us an idea of the class distribution and the structure of the data. Hint: You can use pandas.read_csv. The file might be tab-separated; if so, use sep='\\t'. You may need to specify an encoding (such as 'latin-1') if you encounter errors reading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f5b5853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Label                                            Message\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "Label\n",
      "ham     4825\n",
      "spam     747\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Loading the dataset and initial exploration\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Load the dataset from spam.csv into a DataFrame\n",
    "# (The dataset file \"spam.csv\" is assumed to be in the current directory.)\n",
    "# TODO: Read the CSV file into a pandas DataFrame named df with columns [\"Label\", \"Message\"].\n",
    "# Hint: If using pd.read_csv, consider specifying sep='\\t' and encoding='latin-1'.\n",
    "# If the CSV has no header row, use header=None and names=[\"Label\", \"Message\"].\n",
    "df = pd.read_csv(\"spam.csv\", sep=',', encoding='latin-1')\n",
    "\n",
    "# 2. Display the first 5 rows of the DataFrame to inspect the format\n",
    "# YOUR CODE HERE\n",
    "df = df.iloc[:, :2]\n",
    "df.columns = [\"Label\", \"Message\"]\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# 3. Compute the number of messages that are ham vs. spam\n",
    "# YOUR CODE HERE\n",
    "print(df[\"Label\"].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da844bd",
   "metadata": {},
   "source": [
    "## Question 2: Preprocessing - Tokenization and Normalization\n",
    "Raw text data is messy. To use it in our classifier, we should preprocess the messages:\n",
    "Tokenization: split each message into individual words (tokens).\n",
    "Normalization: convert text to a standard form, e.g., lowercasing all words.\n",
    "Punctuation removal: remove or ignore punctuation so it doesn't count as part of words.\n",
    "\n",
    "In this question, you'll implement a function to preprocess a single message. The preprocessing should:\n",
    "1. Convert the text to lowercase.\n",
    "2. Remove punctuation (you can remove any character that is not a letter or number).\n",
    "3. Split the text into tokens (words).\n",
    "\n",
    "We'll start without removing stop words (common words like \"the\", \"and\", etc.); we'll address that in the next question. Your tasks:\n",
    "\n",
    "Implement the function preprocess_text(text, remove_stopwords=False):\n",
    "Lowercase the input text.\n",
    "Remove punctuation from the text.\n",
    "Split the text into tokens (for example, using str.split() or regular expressions to split on whitespace).\n",
    "For now, ignore the remove_stopwords parameter (we will use it in Question 3).\n",
    "Use the function on the messages in the dataset to create a new column (e.g., \"Tokens\") in the DataFrame containing the list of tokens for each message.\n",
    "Print a sample message and its token list to verify the preprocessing is working as expected.\n",
    "\n",
    "Hints:\n",
    "You can use Python's re module (e.g., re.sub) to remove punctuation by replacing non-alphanumeric characters with a space or empty string.\n",
    "Alternatively, you can remove punctuation by checking each character (.isalnum()).\n",
    "After cleaning, split on whitespace to get tokens.\n",
    "Make sure not to remove spaces between words unintentionally when removing punctuation (replacing punctuation with a space can help separate words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19edfb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original message: Ok lar... Joking wif u oni...\n",
      "Tokens: ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    \"\"\"\n",
    "    Tokenize and normalize the given text.\n",
    "    - Lowercase the text\n",
    "    - Remove punctuation/non-alphanumeric characters\n",
    "    - Split into tokens (words)\n",
    "    If remove_stopwords=True, we'll also remove common stop words (we'll handle this in the next step).\n",
    "    \"\"\"\n",
    "    # Lowercase the text\n",
    "    # Remove punctuation (replace non-letter/number characters with space)\n",
    "    # Split text into tokens\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9]', ' ', text)\n",
    "    tokens = text.split()\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "# Create a new column in the DataFrame for the tokenized messages\n",
    "# Use the preprocess_text function on each message in df[\"Message\"]\n",
    "# YOUR CODE HERE\n",
    "df['Tokens'] = df['Message'].apply(preprocess_text)\n",
    "\n",
    "# Test the preprocessing on a sample message\n",
    "sample_idx = 1  # we'll test on the first message\n",
    "print(\"Original message:\", df.loc[sample_idx, \"Message\"])\n",
    "print(\"Tokens:\", df.loc[sample_idx, \"Tokens\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe86775b",
   "metadata": {},
   "source": [
    "## Question 3: Removing Stop Words\n",
    "\n",
    "Stop words are common words (like \"the\", \"and\", \"to\", \"is\") that may not be useful for distinguishing between spam and ham. Removing stop words can sometimes improve a model's performance by focusing on more meaningful words. In this question, we'll add stop word removal to our preprocessing and consider its effect on the classifier. \n",
    "\n",
    "Your tasks:\n",
    "1. Define a list or set of English stop words. You can use a predefined list (for example, use sklearn's list: from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS) or define a small list of your own (e.g., [\"the\", \"a\", \"an\", \"to\", \"is\", \"in\", ...]).\n",
    "2. Update your preprocess_text function to remove stop words when remove_stopwords=True. This means filtering out tokens that are in your stop words list.\n",
    "3. Test your updated preprocessing on a sample message by calling preprocess_text with remove_stopwords=True and verify that common words are removed.\n",
    "4. Think: (No code required for this part) How might removing stop words affect the performance of the spam classifier? Will it increase, decrease, or have minimal effect on accuracy? We will later evaluate the model with and without stop words to see the difference.\n",
    "\n",
    "Hint: Converting your stop word list to a set will make the membership check (word in stop_words) more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e6ff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without stopword removal: ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n",
      "Tokens with stopword removal: ['ok', 'lar', 'joking', 'wif', 'u', 'oni']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "stop_words = set(ENGLISH_STOP_WORDS)\n",
    "\n",
    "def preprocess_text(text, remove_stopwords=False):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove punctuation and non-alphanumeric characters\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)\n",
    "\n",
    "    \n",
    "    # Split into tokens\n",
    "    tokens = text.split()\n",
    "    \n",
    "    # If remove_stopwords is True, filter out tokens that are in the stop_words set\n",
    "    if remove_stopwords:\n",
    "        tokens = [w for w in tokens if w not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Let's test the function on the same sample message with and without stop word removal\n",
    "sample_text = df.loc[sample_idx, \"Message\"]\n",
    "print(\"Tokens without stopword removal:\", preprocess_text(sample_text, remove_stopwords=False))\n",
    "print(\"Tokens with stopword removal:\", preprocess_text(sample_text, remove_stopwords=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1f6d22",
   "metadata": {},
   "source": [
    "## Question 4: Creating Word Frequency Counts per Class\n",
    "\n",
    "Now that we can tokenize our messages, let's prepare the data for the Naive Bayes classifier. Naive Bayes for text classification uses the frequency of each word in each class (spam or ham) to compute probabilities. In this step, we will:\n",
    "\n",
    "1. Separate the training data into spam messages and ham messages.\n",
    "2. Count how many times each word appears in spam messages and in ham messages.\n",
    "\n",
    "These word frequency counts per class will form the basis of our likelihood estimates $P(\\text{word}|\\text{spam})$ and $P(\\text{word}|\\text{ham})$. Your tasks:\n",
    "1. Split the dataset into a training set and a test set. Use 80% of the data for training and 20% for testing. (It's important to evaluate on unseen test data.)\n",
    "You can use sklearn.model_selection.train_test_split with a fixed random_state (for reproducibility), or shuffle and split manually.\n",
    "Make sure to separate both the messages and labels for train and test.\n",
    "2. Using the training set only, create two dictionaries (or collections.Counter):\n",
    "spam_word_counts: counts of each word in all spam messages in the training set.\n",
    "ham_word_counts: counts of each word in all ham messages in the training set.\n",
    "3. Also compute the total number of words in spam messages (N_spam_words) and in ham messages (N_ham_words) in the training set. (This is the sum of the counts in each dictionary, or equivalently the total length of all spam/ham tokens.)\n",
    "4. Print the top 5 most frequent words in spam_word_counts and ham_word_counts to see some common words in each class (optional, for curiosity).\n",
    "\n",
    "Hint:\n",
    "If you used a DataFrame, you can create train_df and test_df by splitting df. For example, using train_test_split from sklearn.\n",
    "To count words, iterating through each training message's tokens and updating counts is straightforward. You can use dict or Counter.\n",
    "Example with Counter: from collections import Counter; then for spam: spam_word_counts = Counter(), and update it for each spam message's token list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4fd22f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words in spam messages: 10387\n",
      "Total words in ham messages: 30103\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "# 1. Split the full dataset into training and testing sets (e.g., 80% train, 20% test)\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "# Reset indices for convenience\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "# 2. Separate training data into spam and ham subsets\n",
    "# YOUR CODE HERE: create train_spam and train_ham DataFrames\n",
    "train_spam = train_df[train_df[\"Label\"] == \"spam\"]\n",
    "train_ham = train_df[train_df[\"Label\"] == \"ham\"]\n",
    "\n",
    "# 3. Tokenize all messages in each subset (using preprocess_text with remove_stopwords=True)\n",
    "# Create a list of token lists for spam messages and another for ham messages.\n",
    "# YOUR CODE HERE: generate spam_tokens_list and ham_tokens_list\n",
    "spam_tokens_list = []\n",
    "for msg in train_spam[\"Message\"]:\n",
    "    spam_tokens_list.append(preprocess_text(msg, remove_stopwords=True))\n",
    "\n",
    "ham_tokens_list = []\n",
    "for msg in train_ham[\"Message\"]:\n",
    "    ham_tokens_list.append(preprocess_text(msg, remove_stopwords=True))\n",
    "\n",
    "# 4. Count word frequencies for spam and ham\n",
    "spam_word_counts = Counter()\n",
    "ham_word_counts = Counter()\n",
    "\n",
    "# Update the counters with each message's tokens\n",
    "for tokens in spam_tokens_list:\n",
    "    spam_word_counts.update(tokens)\n",
    "for tokens in ham_tokens_list:\n",
    "    ham_word_counts.update(tokens)\n",
    "\n",
    "# 5. Calculate total number of words in spam and ham messages\n",
    "N_spam_words = sum(spam_word_counts.values())\n",
    "N_ham_words = sum(ham_word_counts.values())\n",
    "\n",
    "\n",
    "# 6. Print the total word counts in each class\n",
    "print(f\"Total words in spam messages: {N_spam_words}\")\n",
    "print(f\"Total words in ham messages: {N_ham_words}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cba420",
   "metadata": {},
   "source": [
    "## Question 5: Calculating Prior and Conditional Probabilities\n",
    "Now we will compute the probabilities needed for the Naive Bayes classifier:\n",
    "1. Prior probabilities: $P(\\text{spam})$ and $P(\\text{ham})$ — the probabilities that any given message is spam or ham, based on the training data.\n",
    "2. Conditional probabilities (likelihoods): $P(w|\\text{spam})$ and $P(w|\\text{ham})$ for each word $w$ — the probability of a word appearing in a message given the message is spam or ham.\n",
    "\n",
    "Using the training set:\n",
    "1. $P(\\text{spam}) = \\frac{\\text{count of spam messages in training}}{\\text{total count of messages in training}}$.\n",
    "2. $P(\\text{ham}) = \\frac{\\text{count of ham messages in training}}{\\text{total count of messages in training}}$.\n",
    "\n",
    "For a given word $w$:\n",
    "1. $P(w|\\text{spam}) = \\frac{\\text{count of $w$ in spam messages}}{\\text{total words in spam messages}}$ (using the frequencies you calculated).\n",
    "2. $P(w|\\text{ham}) = \\frac{\\text{count of $w$ in ham messages}}{\\text{total words in ham messages}}$.\n",
    "\n",
    "However, note that if a word did not appear in spam (count = 0), this probability will be 0, which can be problematic. We'll address that in the next question (Laplace smoothing). For now, we'll compute the probabilities without smoothing. \n",
    "\n",
    "Your tasks:\n",
    "1. Calculate the prior probabilities P_spam and P_ham from the training data. Store them in variables P_spam and P_ham.\n",
    "2. Implement a function conditional_prob(word, class_label) that returns $P(word | class_label)$ using the frequency counts from Question 4 (no smoothing yet):\n",
    "If class_label is \"spam\", use spam_word_counts and N_spam_words.\n",
    "If class_label is \"ham\", use ham_word_counts and N_ham_words.\n",
    "If the word is not found in the respective dictionary, the probability should be 0 (since count=0).\n",
    "3. Test your function on a couple of words, for example:\n",
    "A word you expect to be common in spam (like \"free\") for both spam and ham classes.\n",
    "A word that is common in ham or appears in both.\n",
    "4. Print the prior probabilities and a few example conditional probabilities.\n",
    "\n",
    "Hint: The sum of spam_word_counts.values() we computed is the denominator for $P(w|\\text{spam})$. Use integer counts for numerator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9dc8a97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(spam): 0.1339\n",
      "P(ham): 0.8661\n",
      "\n",
      "For word 'free':\n",
      "P(free|spam) = 0.017714\n",
      "P(free|ham) = 0.001628\n",
      "\n",
      "For word 'call':\n",
      "P(call|spam) = 0.000000\n",
      "P(call|ham) = 0.000000\n",
      "\n",
      "For word 'to':\n",
      "P(to|spam) = 0.000000\n",
      "P(to|ham) = 0.000000\n"
     ]
    }
   ],
   "source": [
    "# Number of spam and ham messages in the training set\n",
    "# (We can get these from the lengths of train_spam and train_ham DataFrames)\n",
    "# YOUR CODE HERE: compute num_spam_messages and num_ham_messages\n",
    "num_spam_messages = len(train_spam)\n",
    "num_ham_messages = len(train_ham)\n",
    "num_messages = len(train_df)\n",
    "\n",
    "# Prior probabilities P(spam) and P(ham)\n",
    "P_spam = num_spam_messages / num_messages\n",
    "P_ham = num_ham_messages / num_messages\n",
    "\n",
    "\n",
    "# Print the prior probabilities\n",
    "print(f\"P(spam): {P_spam:.4f}\")\n",
    "print(f\"P(ham): {P_ham:.4f}\")\n",
    "\n",
    "# Conditional probability function for a word given class\n",
    "def conditional_prob(word, class_label):\n",
    "    \"\"\"Return P(word | class_label) based on the training data frequencies.\"\"\"\n",
    "    if class_label == \"spam\":\n",
    "        count = spam_word_counts.get(word, 0)\n",
    "        return count / N_spam_words if N_spam_words > 0 else 0.0\n",
    "    elif class_label == \"ham\":\n",
    "        count = ham_word_counts.get(word, 0)\n",
    "        return count / N_ham_words if N_ham_words > 0 else 0.0\n",
    "    else:\n",
    "        raise ValueError(\"Invalid class_label. Choose 'spam' or 'ham'.\")\n",
    "\n",
    "# Test the conditional probabilities for some example words\n",
    "test_words = [\"free\", \"call\", \"to\"]\n",
    "for w in test_words:\n",
    "    print(f\"\\nFor word '{w}':\")\n",
    "    print(f\"P({w}|spam) = {conditional_prob(w, 'spam'):.6f}\")\n",
    "    print(f\"P({w}|ham) = {conditional_prob(w, 'ham'):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f325f74",
   "metadata": {},
   "source": [
    "## Question 6: Applying Laplace Smoothing\n",
    "\n",
    "To handle zero probabilities for unseen words, we apply **Laplace smoothing** (also known as *add-one smoothing*).  \n",
    "The idea is to pretend we saw each word at least once in each class, so that no probability is ever zero.\n",
    "\n",
    "We compute smoothed conditional probabilities as follows:\n",
    "\n",
    "$$\n",
    "P_{\\text{smooth}}(w \\mid \\text{spam}) = \\frac{\\text{count}(w, \\text{spam}) + 1}{N_{\\text{spam\\_words}} + |V|}\n",
    "$$\n",
    "\n",
    "$$\n",
    "P_{\\text{smooth}}(w \\mid \\text{ham}) = \\frac{\\text{count}(w, \\text{ham}) + 1}{N_{\\text{ham\\_words}} + |V|}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- \\( |V| \\) is the number of **unique words** in the training set vocabulary (the union of all words in spam and ham).\n",
    "- \\( N_{\\text{spam\\_words}} \\) is the total number of words in all spam messages.\n",
    "- \\( N_{\\text{ham\\_words}} \\) is the total number of words in all ham messages.\n",
    "\n",
    "By adding 1 to all word counts, even words that weren’t seen (`count = 0`) will have a small, non-zero probability:\n",
    "\n",
    "$$\n",
    "\\frac{1}{N + |V|}\n",
    "$$\n",
    "\n",
    "Tasks\n",
    "\n",
    "1. Calculate the vocabulary size V (number of unique words in the training set). You can get this by taking the set union of keys from spam_word_counts and ham_word_counts, or by combining the lists of spam and ham tokens.\n",
    "\n",
    "2. Implement a new function conditional_prob_smooth(word, class_label) that returns the Laplace-smoothed probability of a word given class:\n",
    "Use the formulas above: numerator is count + 1, denominator is total words in class + V.\n",
    "Use V you computed for the denominator.\n",
    "\n",
    "3. Test this function on a word that was previously unseen in one class to ensure it's not zero. For example, pick a word that appears in ham but not in spam and check conditional_prob_smooth(word, \"spam\") (it should now be > 0).\n",
    "\n",
    "4. Compare a couple of values from conditional_prob vs conditional_prob_smooth for words that have zero counts to see the difference.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5daf293a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size (training): 7459\n",
      "Word 'boat' is unseen in spam training data.\n",
      "P(boat|spam) without smoothing = 0.000000\n",
      "P(boat|spam) with smoothing = 0.000056\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Compute the vocabulary size (number of unique words across spam and ham)\n",
    "# Hint: Combine keys from spam_word_counts and ham_word_counts into a set\n",
    "\n",
    "# TODO: Create a set called `vocab` that contains all unique words in training\n",
    "# YOUR CODE HERE\n",
    "vocab = set(spam_word_counts.keys()).union(set(ham_word_counts.keys()))\n",
    "V = len(vocab)\n",
    "print(f\"Vocabulary size (training): {V}\")\n",
    "\n",
    "# Step 2: Define a function to compute conditional probabilities with Laplace smoothing\n",
    "# TODO: return conditional_prob_smooth for spam and ham\n",
    "\n",
    "def conditional_prob_smooth(word, class_label):\n",
    "    \"\"\"Return P(word | clzass) with Laplace smoothing.\"\"\"\n",
    "    # TODO: Complete the function for both 'spam' and 'ham'\n",
    "    if class_label == \"spam\":\n",
    "        count = spam_word_counts.get(word, 0)\n",
    "        total_words = sum(spam_word_counts.values())\n",
    "        return (count + 1) / (total_words + V)\n",
    "    elif class_label == \"ham\":\n",
    "        count = ham_word_counts.get(word, 0)\n",
    "        total_words = sum(ham_word_counts.values())\n",
    "        return (count + 1) / (total_words + V)\n",
    "    else:\n",
    "        raise ValueError(\"class_label must be 'spam' or 'ham'\")\n",
    "\n",
    "# Step 3: Pick a word that is in ham messages but not in spam messages\n",
    "# This will help us compare smoothed vs unsmoothed probabilities\n",
    "\n",
    "# TODO: Loop over ham_word_counts to find a word not seen in spam_word_counts\n",
    "unseen_in_spam = None\n",
    "\n",
    "for word in ham_word_counts:\n",
    "    if word not in spam_word_counts:\n",
    "        unseen_in_spam = word\n",
    "        break\n",
    "\n",
    "# Print results using both smoothed and unsmoothed functions\n",
    "if unseen_in_spam:\n",
    "    print(f\"Word '{unseen_in_spam}' is unseen in spam training data.\")\n",
    "    print(f\"P({unseen_in_spam}|spam) without smoothing = {conditional_prob(unseen_in_spam, 'spam'):.6f}\")\n",
    "    print(f\"P({unseen_in_spam}|spam) with smoothing = {conditional_prob_smooth(unseen_in_spam, 'spam'):.6f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d322970",
   "metadata": {},
   "source": [
    "# Question 7: Implementing the Naive Bayes Classifier\n",
    "\n",
    "Using the probabilities we derived, we can now classify a new message as spam or ham.\n",
    "\n",
    "According to Bayes’ theorem, we compute:\n",
    "\n",
    "$$\n",
    "P(\\text{spam} \\mid \\text{message}) \\propto P(\\text{spam}) \\prod_{w \\in \\text{message}} P(w \\mid \\text{spam})\n",
    "$$\n",
    "\n",
    "$$\n",
    "P(\\text{ham} \\mid \\text{message}) \\propto P(\\text{ham}) \\prod_{w \\in \\text{message}} P(w \\mid \\text{ham})\n",
    "$$\n",
    "\n",
    "To avoid numerical underflow from multiplying many small probabilities, we work in the **log space**. This gives us the **log-likelihoods**:\n",
    "\n",
    "$$\n",
    "\\log P(\\text{spam} \\mid \\text{message}) = \\log P(\\text{spam}) + \\sum_{w \\in \\text{message}} \\log P(w \\mid \\text{spam})\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\log P(\\text{ham} \\mid \\text{message}) = \\log P(\\text{ham}) + \\sum_{w \\in \\text{message}} \\log P(w \\mid \\text{ham})\n",
    "$$\n",
    "\n",
    "We then predict the class with the higher log-probability.\n",
    "\n",
    "---\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "- Implement a function `predict_naive_bayes(message)` that:\n",
    "  - Preprocesses the input message using your `preprocess_text` function.\n",
    "  - Computes the **log-probability** of the message being spam and ham using Laplace-smoothed conditional probabilities.\n",
    "  - Returns the class `\"spam\"` if the spam log-probability is higher, otherwise `\"ham\"`.\n",
    "\n",
    "- Test your function with two example messages:\n",
    "  - `\"Congratulations! You've won a free lottery. Call now to claim $$$\"` (likely spam)\n",
    "  - `\"Hey, are we still on for dinner tonight?\"` (likely ham)\n",
    "\n",
    "---\n",
    "\n",
    "Hint:\n",
    "Use `math.log()` for computing logarithms, and be sure your probabilities are nonzero (you should use the Laplace-smoothed ones).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca54b8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: Congratulations! You've won a free lottery. Call now to claim $$$\n",
      "Predicted: spam\n",
      "---\n",
      "Message: Hey, are we still on for dinner tonight?\n",
      "Predicted: ham\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Question 7: Naive Bayes prediction function\n",
    "\n",
    "def predict_naive_bayes(message):\n",
    "    \"\"\"\n",
    "    Predict whether a given message is \"spam\" or \"ham\" using the trained Naive Bayes model.\n",
    "    Returns the predicted label as a string.\n",
    "    \"\"\"\n",
    "    # Step 1: Preprocess the message\n",
    "    # TODO: Tokenize and clean the message\n",
    "    tokens = preprocess_text(message)\n",
    "\n",
    "    # Step 2: Initialize log probabilities with prior probabilities\n",
    "    # TODO: Use math.log on the prior probabilities P_spam and P_ham\n",
    "    log_prob_spam = math.log(P_spam)\n",
    "    log_prob_ham = math.log(P_ham)\n",
    "\n",
    "    # Step 3: Add log conditional probabilities for each word\n",
    "    for word in tokens:\n",
    "        if word in vocab:\n",
    "            # TODO: Add log P(word | spam) and log P(word | ham)\n",
    "            log_prob_spam += math.log(conditional_prob_smooth(word, \"spam\"))\n",
    "            log_prob_ham += math.log(conditional_prob_smooth(word, \"ham\"))\n",
    "        # If the word is not in the vocabulary, skip it\n",
    "\n",
    "    # Step 4: Return the class with the higher log probability\n",
    "    # TODO: Compare log_prob_spam and log_prob_ham\n",
    "    if log_prob_spam > log_prob_ham:\n",
    "        return \"spam\"\n",
    "    else:\n",
    "        return \"ham\"\n",
    "\n",
    "# Quick tests for the predictor\n",
    "test_messages = [\n",
    "    \"Congratulations! You've won a free lottery. Call now to claim $$$\",  # likely spam\n",
    "    \"Hey, are we still on for dinner tonight?\",  # likely ham\n",
    "]\n",
    "for msg in test_messages:\n",
    "    print(f\"Message: {msg}\")\n",
    "    print(f\"Predicted: {predict_naive_bayes(msg)}\")\n",
    "    print(\"---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5333fc5a",
   "metadata": {},
   "source": [
    "# Question 8: Making Predictions on the Test Set\n",
    "\n",
    "With our classifier function ready, we can now apply it to every message in the test set and see how well it performs on unseen data. In this step, you'll generate predictions for the test set and compare them to the actual labels. \n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Use the predict_naive_bayes function to predict labels for each message in test_df.\n",
    "You can do this with a loop, or by using pandas apply on the \"Message\" column of test_df.\n",
    "2. Store the predictions in a list or as a new column in test_df (e.g., test_df[\"Predicted\"]).\n",
    "3. Print the first 10 predictions alongside the true labels to get a sense of how the classifier is doing.\n",
    "4. (Important for grading) Also create two lists or arrays:\n",
    "y_true for the true labels in the test set,\n",
    "y_pred for the predicted labels.\n",
    "\n",
    "We'll use y_true and y_pred in the next step to compute evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5bb0f108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample predictions (Actual -> Predicted):\n",
      "ham -> ham\n",
      "ham -> ham\n",
      "spam -> spam\n",
      "ham -> ham\n",
      "spam -> spam\n",
      "ham -> ham\n",
      "ham -> ham\n",
      "ham -> ham\n",
      "ham -> ham\n",
      "ham -> ham\n"
     ]
    }
   ],
   "source": [
    "# Question 8: Predict on test set\n",
    "\n",
    "# Generate predictions for each message in the test set\n",
    "y_true = list(test_df[\"Label\"])\n",
    "y_pred = []  # this will hold our predicted labels\n",
    "\n",
    "# Loop over each message in test set and predict\n",
    "for msg in test_df[\"Message\"]:\n",
    "    pred_label = predict_naive_bayes(msg)\n",
    "    y_pred.append(pred_label)\n",
    "\n",
    "# Optionally, add predictions to the test DataFrame for convenience\n",
    "test_df[\"Predicted\"] = y_pred\n",
    "\n",
    "# Print first 10 results: actual vs predicted\n",
    "print(\"Sample predictions (Actual -> Predicted):\")\n",
    "for i in range(10):\n",
    "    actual = y_true[i]\n",
    "    predicted = y_pred[i]\n",
    "    print(f\"{actual} -> {predicted}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33a8a5",
   "metadata": {},
   "source": [
    "# Question 9: Evaluating Model Performance\n",
    "\n",
    "Now that we have predictions on the test set, let's evaluate how well our Naive Bayes classifier performed. We will calculate the following evaluation metrics:\n",
    "1. Accuracy: the proportion of messages correctly classified.\n",
    "2. Precision (for the \"spam\" class): among the messages predicted as spam, what fraction are actually spam.\n",
    "3. Recall (for the \"spam\" class): among the actual spam messages, what fraction did the classifier correctly identify as spam.\n",
    "4. F1-score: the harmonic mean of precision and recall, giving a single measure of classifier quality for the positive class.\n",
    "For these metrics, we'll treat \"spam\" as the positive class. (It's common in binary classification to focus on the performance for the positive class of interest, here spam detection.) \n",
    "\n",
    "Your tasks:\n",
    "1. Calculate the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) by comparing y_pred and y_true:\n",
    "TP: cases where the true label is spam and the model predicted spam.\n",
    "TN: cases where the true label is ham and the model predicted ham.\n",
    "FP: cases where the true label is ham but the model predicted spam (a ham message incorrectly flagged as spam).\n",
    "FN: cases where the true label is spam but the model predicted ham (a spam message missed by the classifier).\n",
    "2. Using TP, TN, FP, FN, compute:\n",
    "accuracy = (TP + TN) / total_test_messages\n",
    "precision = TP / (TP + FP) (if TP+FP is 0, set precision to 0 to avoid division by zero).\n",
    "recall = TP / (TP + FN) (if TP+FN is 0, set recall to 0).\n",
    "f1 = 2 * precision * recall / (precision + recall) (if precision+recall is 0, then F1 can be set to 0).\n",
    "3. Print out the four metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eeca39bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.9848\n",
      "Precision: 0.9716\n",
      "Recall:    0.9133\n",
      "F1-score:  0.9416\n"
     ]
    }
   ],
   "source": [
    "# Initialize counts\n",
    "TP = FP = TN = FN = 0\n",
    "\n",
    "for actual, pred in zip(y_true, y_pred):\n",
    "    if actual == \"spam\" and pred == \"spam\":\n",
    "        TP += 1\n",
    "    elif actual == \"ham\" and pred == \"ham\":\n",
    "        TN += 1\n",
    "    elif actual == \"ham\" and pred == \"spam\":\n",
    "        FP += 1\n",
    "    elif actual == \"spam\" and pred == \"ham\":\n",
    "        FN += 1\n",
    "\n",
    "\n",
    "# Compute metrics\n",
    "total = TP + TN + FP + FN\n",
    "\n",
    "accuracy = (TP + TN) / total if total > 0 else 0\n",
    "precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "print(f\"Accuracy:  {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-score:  {f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727ac42f",
   "metadata": {},
   "source": [
    "# Question 10: Interpreting Results and Discussing Limitations\n",
    "\n",
    "Finally, let's interpret our results and reflect on the Naive Bayes classifier:\n",
    "1. Performance Discussion: Look at the accuracy, precision, recall, and F1 you obtained. Are they satisfactory for a spam filter? Did the model perform better at identifying ham vs spam (check precision and recall values for spam)? For example, if precision is high but recall is low, the filter rarely flags ham as spam (good) but misses some spam (not ideal). Share your observations.\n",
    "2. Effect of Stop Words Removal: If you removed stop words in preprocessing, do you think it helped the model? (You could compare with a run without stop word removal to see the difference, if time permits.) Typically, removing stop words might slightly improve or have minimal effect on spam detection, since stop words are common in both ham and spam and don't carry much discriminatory power. Briefly explain your reasoning or findings.\n",
    "3. Limitations of Naive Bayes: Discuss some limitations of the Naive Bayes approach for text classification in this context:\n",
    "Independence assumption: Naive Bayes assumes that words in a message occur independently given the class, which is not strictly true (e.g., phrases or word combinations aren't considered).\n",
    "Misleading evidence: If a ham message contains a very \"spammy\" word or vice versa, Naive Bayes will weigh that word heavily, possibly ignoring context.\n",
    "Data sparsity: If a spam has words never seen before (which we handled with smoothing), the model might still be unsure. Naive Bayes doesn't capture semantics—two different words are completely unrelated to it (e.g., \"prize\" and \"award\" are treated as distinct features with no relationship).\n",
    "Other limitations: It's a simple model that might not catch more subtle patterns (like character obfuscation in spam \"cl1ck here\", or the overall structure of messages).\n",
    "\n",
    "Your tasks:\n",
    "\n",
    "1. Write a short discussion (3-5 sentences for each point above) interpreting the metrics and discussing the effect of stop word removal and limitations of Naive Bayes for spam detection.\n",
    "2. Please provide your answer in the markdown cell below (no code needed here, just explanation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99aea38f",
   "metadata": {},
   "source": [
    "**Answer:**\n",
    "\n",
    "*Performance: Achieved an accuracy of 98.5%, precision of 97.2%, recall of 91.3%, and an F1-score of 94.2%, High 90's suggest that overall performance was good. The high precision means we are able to identify what is spam without mislabeling. The  lower recall suggests that it occasionally misses spam messages (false negatives), which could be improved. Overall, the balance between precision and recall is fairly good and deems reliable.\n",
    "\n",
    "\n",
    "*Effect of Stop Words Removal: By removing stop words, the model can focus on more important words which are more useful for classification. Helpsreduce noise and improve precision and recall.\n",
    "\n",
    "\n",
    "*Limitations of Naive Bayes: Naive Bayes thinks each word is independent, which isn’t always true in real messages. It doesn’t understand word order or meaning, as humans understand context. It can also struggle with words it hasn’t seen before, it’s a interesting model that works well but has shown to not be a 100% accurate.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
